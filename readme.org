Redesign the umat in python
--------------------------


* Why are we doing this?
** How UMAT works
What happens in UMAT is simple: at its heart, the return-mapping-algorithm, solves a nonlinear system using the NR procedure. There are 48 unknowns/equations in this system. 24 are the amount of (plastic) slip and 24 are the plastic slip resistances. Because of the nonlinear nature of this system of equations, we solve this system by perturbing on unnkown at a time and form a jacobian, which we will use to update the values of the unknowns while iterating.

** What we hope to achieve
- If we can identify the forward pass in the UMAT (the one that we use as a starting point for perturbation), then we can implement that in pytorch. Then we can investigate if this forward pass will give us a gradient. If yes, we use this gradient to multiply with the residual vector


** can pytorch compute the inverse of a jacobian?
If the forward pass is successful, we get the jacobian, but to complete the NR-procedure, we will need to multiply the inverse of this jacobian with the residual vector. Can we do this inversion? 


* code coverage
coverage run -m unittest discover
coverage report -m

* List of code modification
** DONE Implement the inverse gamma relation, as mentioned in Denny's thesis.
Implemented. Currently does not yield good results. It could easily result in nan values and currently is not used. At the moment relying on penalties and data loss to guide the search. 
*** DONE Correct get_r_II function. In case of g^{i} < s^{i}, we should have \Delta\gamma^{i} and not zero.
  This is already taken care of with the inclusion of inverse-gamma relation. 
** TODO Implement barrier penalty formulation. 
** DONE Factor drive.py into smaller routines.
** TODO Run UMAT-Fortran with the same orientation as the one we use in python-umat and collect statistics
*** Compare stress prior to yield from both methods
**** Implement stress calculation (Cauchy) in python-umat.
*** How many iterations are done in RETURNMAPPINGBCC
*** Residual at the end of the return mapping 
*** How many times we call slip-system-check routine
*** Time that takes for a single run of return mapping algorithm
** DONE Make the code flexible enough so it runs with optimizers from optim modules, the ad-hoc optimizer, where we zero parameters (that violate constraints) after each iteration.
 This is not considered anymore since we have shifted to PINNs framework.
** DONE Find small numerical "constrained optimization problems" to test various optimization algorithms.
 This is not considered anymore since we have shifted to PINNs framework.
** DONE Write more tests
All the umat routine are tested and verified (constants, construction and batch computation). Additional tests are written to ensure the computation in fortran and pytorch result in the same values. 
** DONE Read about L1-regularization (Could it lead to sparse parameter set?)
A very wishy-washy explanation, based on what I took from Goodfelllow's book:
 The main effect of L1-regularization is that the gradient does not scale with the value of parameters, as is the case with L2-regularization. Instead, we get unit gradients for each parameter of the optimization. This, combine with a large penalty coefficient, pushes the parameter to the boundaries (or beyond, which in that case, it is being projected back to the boundary) of the subspace that defines the permissible space for optimization. In short, there is a higher chance, although not guaranteed, that L1-regularization will lead to sparse parameter set than L2-regularization.
** DONE Try LBFGS 
 - How to include constraints in the formulation of LBFGS? This won't be needed at the moment since our focus has shifted to compute the loss using PINNs approach. As such, solely optimizing the physical loss is no longer considered. 

** DONE Add frequency for logging required data into tensorboard. Make sure that frequencies are provided for each logging item.

** TODO Try both gamma_dot equation in its original form and its inverse form. Add this to config_file as a switch

** TODO Replay buffer code. 

** DONE Autoregression code should run from any point in the code (parameter alpha which is between 0 and 1)
ACTION:
   Parameter alpha is an argument to the inference funcion now. Tested and works fine.
** TODO Autoregression computes both stress and gamma/slipres values. Find proper error indicators for gamma.

** TODO Make sure that the computation of stresses from gamma/slip values is consistent and error free.

** DONE Submodule the UMAT/Fortran repo to use it for inference.

** DONE At inference time, we need to clip gamma and slip resistance that we get from the model
 - delta gamma should be strictly non-negative
 - slip resistance should be clipped between min and max slip resistance
ACTION:   
   Both of these are implemented at both train and inference code. 
** TODO How to deal with multiple, competing losses.
*** TODO Implement a warmup period for where physics is not calculated. There are two approached to this:
 1. Try to see if relative error make sense. For example, as long as the physics loss is bigger than 10x data_loss don't compute the physics loss
 2. Simply skip computing the physics loss for eg 100 iterations.
*** TODO Clipping the gardient. Clip gradient of each loss separately. But for this we need to accumulate the gradients individually from each loss without the help from the optimizer. See how this should be implemented.
*** TODO L2 regularization of weights of the network.


** TODO Try different options:
Here `-->` is a place holder for the model. 
 1- gamma_0, s_0, F0, F1, theta --> gamma_1, s_1
 2- gamma_0, s_0, F0, F1, theta --> Delta_gamma, Delta_s
 3- (r_II)_p --> Delta_gamma, Delta_s, where (r_II)_p is the predictor r_II assuming Delta_gamma = 0 and Delta_s = 0. (r_I)_p is zero arbitrarily (r_I = Delta_gamma - H Delta_s) and does not make sense to have it as an input.
*** TODO Before implementing (3), we need to find out how big the predictor vector is.
** DONE For the physics loss, r_I and r_II should be concatenated before calculating the loss.
Currently we have mse(r_I, 0) + mse(r_II, 0). When solving the equations in umat, we consider only one system of equations, comprised of both r_I and r_II. It is therefore appropriate to have one loss when training instead of separating them, ie, mse([r_I, r_II], 0)
** TODO Find out a way to anneal beta for the current situation. `beta` is currently the function of curernt step and total steps, where the latter is undefined due to the way we train the model. One idea is to keep a running average of td_errors and adjust beta based on that.
** TODO 1- Logging
*** DONE save pictures of stress (model vs umat)
*** TODO losses and td_errors. Break down to subcomponents and total
**** DONE loss subcomponents
**** DONE td-error statistics
**** TODO ERB (Experience Replay Buffer) sampled weight statistics
**** TODO Beta values.
*** TODO Make sure beta for ERB is bounded from above to 1.
*** DONE Find out why the logs stop at 5000
*** DONE log the td-buffer statistics
*** DONE Understand the loss calculation
*** TODO plot norm or gamma and slip resistance for validated samples (both from UMAT and autoregressive model)
* TODO 2- Testing
** DONE r_I and r_II should be close to zero.
We checked this already and wrote a test for it to document it. It's under `tests/residual_investigation`
** DONE mechanical stress calculation should be close to umat stress.
Also checked this and confirm that it is done correctly. It's part of the test `tests/residual_investigation`.
** TODO See for example, how the error grows for (r_I)p and (r_II)p when we linearly changing the variables s and gamma from values in t_n to values in t_{n+1}. Particularly interesting is to see how big the (r_I) and (r_II) when computed with gamma=gamma_n and s=s_n. We can define (r_I)_{\alpha} and (r_II)_{\alpha} where \alpha is between 0 and 1 and linearly interpolates between values at t_n and t_{n+1}, ie, gamma = \alpha gamma_n + (1 - \alpha) gamma_{n+1} and s = \alpha s_n + (1 - \alpha) s_{n+1}.
** DONE Study the reason why $r_I$ is Nan. With clipping and enforcing positive delta gamma, we should not get nan for r_I
 - This was due to the way clipping was done. We used to clip slip_resistance to be between min and max slip resistance, but if the updated slip resistance is less than slip_resistance_0, k will be imaginary. So the proper way to clip slip_resistance is to constrain it to be between slip_resistance_0 and max slip resistance, ie,  s_0 <= s_1 <= s_max.
 - Another issue that can arise is to consider the residuals that can arise with respect to floating point additions. This happens when constraining s from above (s_1 <= s_max). So we need to create a buffer zone where we don't really touch the upper limit, ie, s_1 + eps <= s_max.
* DONE Implement learning rate scheduler. Look for variable learning rate such as cosine lr.
Implemetend cosine annealing with warmup that linearly reduces its warmup peak value. For now we can work with this and don't need to invest in more LR schedulers, until we implement a better network and add more loggings.
* TODO Implement netowrks with higher capacity


