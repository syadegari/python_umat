Redesign the umat in python
--------------------------


* Why are we doing this?
** How UMAT works
What happens in UMAT is simple: at its heart, the return-mapping-algorithm, solves a nonlinear system using NR procedure. There are 48 unknowns/equations in this system. 24 are the amount of (plastic) slip and 24 are the plastic slip resistances. Because of the nonlinear nature of this system of equations, we solve this system by perturbing on unnkown at a time and form a jacobian, which we will use to update the values of the unknowns while iterating.

** What we hope to achieve
- If we can identify the forward pass in the UMAT (the one that we use as a starting point for perturbation), then we can implement that in pytorch. Then we can investigate if this forward pass will give us a gradient. If yes, we use this gradient to multiply with the residual vector


** can pytorch compute the inverse of a jacobian?
If the forward pass is successful, we get the jacobian, but to complete the NR-procedure, we will need to multiply the inverse of this jacobian with the residual vector. Can we do this inversion? 


* code coverage
coverage run -m unittest discover
coverage report -m

* List of code modification
** Implement the inverse gamma relation, as mentioned in Denny's thesis
** Correct get_r_II function. In case of g^{i} < s^{i}, we should have \Delta\gamma^{i} and not zero.
** Implement barrier penalty formulation. 
** Factor drive.py into smaller routines.
** Run UMAT-Fortran with the same orientation as the one we use in python-umat and collect statistics
*** Compare stress prior to yield from both methods
**** Implement stress calculation (Cauchy) in python-umat.
*** How many iterations are done in RETURNMAPPINGBCC
*** Residual at the end of the return mapping 
*** How many times we call slip-system-check routine
*** Time that takes for a single run of return mapping algorithm
** Make the code flexible enough so it runs with optimizers from optim modules, the ad-hoc optimizer, where we zero parameters (that violate constraints) after each iteration.
** Find small numerical "constrained optimization problems" to test various optimization algorithms.
** Write more tests
** DONE Read about L1-regularization (Could it lead to sparse parameter set?)
A very wishy-washy explanation, based on what I took from Goodfelllow's book:
 The main effect of L1-regularization is that the gradient does not scale with the value of parameters, as is the case with L2-regularization. Instead, we get unit gradients for each parameter of the optimization. This, combine with a large penalty coefficient, pushes the parameter to the boundaries (or beyond, which in that case, it is being projected back to the boundary) of the subspace that defines the permissible space for optimization. In short, there is a higher chance, although not guaranteed, that L1-regularization will lead to sparse parameter set than L2-regularization.
** Try LBFGS 
*** How to include constraints in the formulation of LBFGS? 

